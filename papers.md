## Deep Metric Learning via Lifted Structured Feature Embedding (`2015`)
### [arxiv 1511.06452](https://arxiv.org/pdf/1511.06452.pdf), [github](https://github.com/rksltnl/Deep-Metric-Learning-CVPR16)

<p align="center"><img src="vis/lse_loss.png" width="500"></p>

* Идея предложенного лосса **(3)** : строим связи внутри целого батча (как всегда: притягивая объекты одного класса и отдяляя объекты разных) — это следующий шаг после contrastive pair и триплета:

<p align="center"><img src="vis/contrast_triplet_lse_diff.png" width="300"></p>

* Как читать лосс:
1. Берем пару из одного класса `(i, j)` — сближаем
2. Рассматриваем `i` в качестве anchor'а и ищем в других классах **самый** hard negative
3. То же самое проделываем для `j`
4. Из 2. и 3. выбираем более сложный hard negative — отдаляем
* Преимущество: в лоссе используется вся информация о микробатче (на самом деле не совсем).
* Преимущество лосса — утилизируется весь микробатч *(возможно, одни из первых)*.
* Проблемы: не дифференцируема и переусложненный майнинг. Первая проблема решается субградиентом, но утверждается, вложенные максимумы все равно приводят к плохим локальным оптимумам *(это не очевидно, но эксперименты не приводятся)*.
* Решения проблем классические: upper bound *(забавно, но тут тоже LSE)* + stochastic sampling **(4)**:

<p align="center"><img src="vis/lse_loss_upper_bnd.png" width="500"></p>

* На самом деле преимущество аппроксимации не только в оптимизации, но и в том, что для негативов градиент идет не по одному very hard семплу. Вместо этого фактически строится распределение hard'овости (градиент пропорционален софтмаксу отклонений) — красиво **(6)**:

<p align="center"><img src="vis/lse_grad.png" width="500"></p>

* Рассматривают следующе развитие лоссов: Contrastive Embedding -> Triplet Embedding -> *Lifted Structured Embedding*
* Мотивация: extreme classification *(наконец нашлелся термин)*.
* Собрали датасет: [Stanford Online Products Dataset](https://cvgl.stanford.edu/projects/lifted_struct/) — 120k картинок, 23k классов — один из наиболее крупных по количеству классов на тот момент; впоследствии использовался для сравнения различных ML-подходов.
* Эксперименты на задаче поиска и кластеризации. Также приводятся эксперименты на CARS-196 и CUB-200. Для всех датасетов половина классов была отправлена на обучени и половина на тест — для замера использовали [F1, NMI и Recall@k](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html).
* Есть детали обучения (но непонятно, почему приводят размеры батча для contrastive и triplet, но не приводят для LSE).

<p align="center"><img src="vis/lse_on_online_priducts.png" width="700"></p>

* На трех рассмотренных задачах LSE был SOTA. Также для подтверждения результатов, кроме функционалов качества, авторы визуализируют t-SNE неизвестных модели классов (анализ ошибок тоже есть).